{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 align=\"center\">__Module 3 Activity__</h3>\n",
    "# <h3 align=\"center\">__Assigned at the start of Module 3__</h3>\n",
    "# <h3 align=\"center\">__Due at the end of Module 3__</h3><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Discussion Forum Participation\n",
    "\n",
    "Each week, you are required to participate in the module’s discussion forum. The discussion forum consists of the week's Module Activity, which is released at the beginning of the module. You must complete/attempt the activity before you can post about the activity and anything that relates to the topic. \n",
    "\n",
    "## Grading of the Discussion\n",
    "\n",
    "### 1. Initial Post:\n",
    "Create your thread by **Day 5 (Saturday night at midnight, PST).**\n",
    "\n",
    "### 2. Responses:\n",
    "Respond to at least two other posts by **Day 7 (Monday night at midnight, PST).**\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Criteria:\n",
    "\n",
    "Your participation will be graded as follows:\n",
    "\n",
    "### Full Credit (100 points):\n",
    "- Submit your initial post by **Day 5.**\n",
    "- Respond to at least two other posts by **Day 7.**\n",
    "\n",
    "### Half Credit (50 points):\n",
    "- If your initial post is late but you respond to two other posts.\n",
    "- If your initial post is on time but you fail to respond to at least two other posts.\n",
    "\n",
    "### No Credit (0 points):\n",
    "- If both your initial post and responses are late.\n",
    "- If you fail to submit an initial post and do not respond to any others.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes:\n",
    "\n",
    "- **Late Initial Posts:** Late posts will automatically receive half credit if two responses are completed on time.\n",
    "- **Substance Matters:** Responses must be thoughtful and constructive. Comments like “Great post!” or “I agree!” without further explanation will not earn credit.\n",
    "- **Balance Participation:** Aim to engage with threads that have fewer or no responses to ensure a balanced discussion.\n",
    "\n",
    "---\n",
    "\n",
    "## Avoid:\n",
    "- A number of posts within a very short time-frame, especially immediately prior to the posting deadline.\n",
    "- Posts that complement another post, and then consist of a summary of that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Activity: Building a Preprocessing Pipeline\n",
    "\n",
    "## Objective\n",
    "Learn how to build a preprocessing pipeline in scikit-learn and apply it to the famous Iris dataset. Gain hands-on experience in handling missing values, scaling features, and understanding the importance of preprocessing pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Sample Code for Pipeline Syntax\n",
    "Here’s an example to help you understand how to create a pipeline. This pipeline imputes missing values using the mean:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': [1.0, np.nan, 3.0],\n",
    "    'Feature2': [np.nan, 2.0, 3.0]\n",
    "})\n",
    "\n",
    "# Define a pipeline with an imputer\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "processed_data = pipeline.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nProcessed Data:\")\n",
    "print(processed_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Instructions\n",
    "\n",
    "## Dataset Preparation\n",
    "We will use the Iris dataset, randomly remove values to simulate missing data, and keep it in a Pandas DataFrame for you to preprocess.\n",
    "\n",
    "---\n",
    "\n",
    "## Your Task\n",
    "Build a preprocessing pipeline that:\n",
    "- Imputes missing values using the median.\n",
    "- Scales features to a `[0, 1]` range using `MinMaxScaler`.\n",
    "- Add at least one more preprocessing step.\n",
    "\n",
    "### Reflection\n",
    "At the end of the activity, answer the following questions:\n",
    "1. What challenges did you face while handling missing data?\n",
    "2. Why is it important to use a pipeline for preprocessing?\n",
    "---\n",
    "\n",
    "## Dataset Setup\n",
    "Run the following code to import the Iris dataset and simulate missing data. You will use this dataset for the activity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with Missing Values:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                NaN               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                NaN               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "5                5.4               3.9                1.7               0.4\n",
      "6                NaN               3.4                1.4               0.3\n",
      "7                5.0               NaN                NaN               0.2\n",
      "8                4.4               2.9                1.4               0.2\n",
      "9                4.9               3.1                1.5               0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Randomly introduce missing values in random cells\n",
    "np.random.seed(42)\n",
    "total_cells = data.size\n",
    "num_missing = int(0.1 * total_cells)  # 10% of total cells\n",
    "missing_indices = [(row, col) for row in range(data.shape[0]) for col in range(data.shape[1])]\n",
    "random_missing_indices = np.random.choice(len(missing_indices), size=num_missing, replace=False)\n",
    "\n",
    "for index in random_missing_indices:\n",
    "    row, col = missing_indices[index]\n",
    "    data.iat[row, col] = np.nan\n",
    "\n",
    "print(\"Dataset with Missing Values:\")\n",
    "print(data.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Build your pipeline** to preprocess the dataset.\n",
    "2. **Test your pipeline** by fitting it to the Iris dataset and transforming it.\n",
    "3. **Review the processed data** and reflect on how the pipeline simplifies your workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # handle missing values by replacing them with the median\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # select the best 2 features based on statistical tests\n",
    "    ('feature_selector', SelectKBest(mutual_info_classif, k=2)),\n",
    "    # scale features to a range\n",
    "    ('scaler', MinMaxScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before processing:\n",
      "   petal length (cm)  petal width (cm)\n",
      "0           0.576271          0.041667\n",
      "1           0.067797          0.041667\n",
      "2           0.576271          0.041667\n",
      "3           0.084746          0.041667\n",
      "4           0.067797          0.041667\n",
      "5           0.118644          0.125000\n",
      "6           0.067797          0.083333\n",
      "7           0.576271          0.041667\n",
      "8           0.067797          0.041667\n",
      "9           0.084746          0.000000\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline with the data and target variable\n",
    "data_preprocessed = pipeline.fit_transform(data, iris.target)\n",
    "\n",
    "# get the names of the selected features\n",
    "selected_features = pipeline.named_steps['feature_selector'].get_feature_names_out(data.columns)\n",
    "\n",
    "# convert the preprocessed data to a DataFrame\n",
    "data_preprocessed = pd.DataFrame(data_preprocessed, columns=selected_features)\n",
    "\n",
    "# display the first 10 rows of the preprocessed data\n",
    "print(\"DataFrame before processing:\")\n",
    "print(data_preprocessed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Review Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after processing:\n",
      "   petal length (cm)  petal width (cm)\n",
      "0           0.576271          0.041667\n",
      "1           0.067797          0.041667\n",
      "2           0.576271          0.041667\n",
      "3           0.084746          0.041667\n",
      "4           0.067797          0.041667\n",
      "5           0.118644          0.125000\n",
      "6           0.067797          0.083333\n",
      "7           0.576271          0.041667\n",
      "8           0.067797          0.041667\n",
      "9           0.084746          0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame after processing:\")\n",
    "print(data_preprocessed.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values (will display 0 if none are found in that column):\n",
      "petal length (cm)    0\n",
      "petal width (cm)     0\n",
      "dtype: int64\n",
      "\n",
      "Min values:\n",
      "petal length (cm)    0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n",
      "\n",
      "Max values:\n",
      "petal length (cm)    1.0\n",
      "petal width (cm)     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values by summing the missing values in each column\n",
    "print(\"Missing values (will display 0 if none are found in that column):\")\n",
    "print(data_preprocessed.isnull().sum())\n",
    "\n",
    "# showcase the min and max values of each column\n",
    "print(\"\\nMin values:\")\n",
    "print(data_preprocessed.min())\n",
    "print(\"\\nMax values:\")\n",
    "print(data_preprocessed.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "## Challenges Faced\n",
    "### \n",
    "I had an issue handling the missing values. I at first tried to feature select beforehand but then realized this was not the optimal way to do it. I realized that it would eventually cause issues later on if the missing values weren't handled first.\n",
    "I also struggled with figuring out if missing values were fully removed. Adding the data.isnull().sum() portion solved this issue.\n",
    "\n",
    "## Importance of Using Pipeline for Preprocessing\n",
    "### \n",
    "It keeps preprocessing organized by automating the steps in a pre-set structure. Data is fit and transformed in one step. Allows the handling of values, scaling, and transforming data to be done in a single process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
